{
  "decision tree": "A Decision Tree is simple, fast, and good for understanding decisions in data.",
  "tree": "A Decision Tree is simple, fast, and good for understanding decisions in data.",
  "svm": "SVM (Support Vector Machine) is powerful for high-dimensional data and binary classification.",
  "support vector": "SVM (Support Vector Machine) is powerful for high-dimensional data and binary classification.",
  "logistic regression": "Logistic Regression is ideal for predicting binary outcomes like yes/no.",
  "logistic": "Logistic Regression is ideal for predicting binary outcomes like yes/no.",
  "linear regression": "Linear Regression is great for predicting continuous values like price, marks, etc.",
  "linear": "Linear Regression is great for predicting continuous values like price, marks, etc.",
  "accuracy": "Accuracy tells how many predictions your model got right.",
  "dataset": "A dataset contains the input features and labels to train ML models.",
  "csv": "A dataset in CSV format is commonly used for training ML models.",
  "train": "Training teaches the model from examples.",
  "test": "Testing checks how well your trained model performs.",
  "training": "Training teaches the model from examples.",
  "testing": "Testing checks how well your trained model performs.",
  "predict": "Prediction in ML means estimating output for new, unseen data.",
  "prediction": "Prediction in ML means estimating output for new, unseen data.",
  "overfitting": "Overfitting is when the model memorizes the training data and performs badly on new data.",
  "underfitting": "Underfitting is when the model is too simple and fails to capture the data patterns.",
  "random forest": "Random Forest builds many decision trees and averages their predictions for better accuracy.",
  "forest": "Random Forest builds many decision trees and averages their predictions for better accuracy.",
  "supervised": "Supervised learning uses input-output pairs (labels) to learn.",
  "unsupervised": "Unsupervised learning finds hidden patterns in data without labels.",
  "ml": "ML stands for Machine Learning. It lets computers learn from data and make predictions.",
  "machine learning": "Machine Learning helps systems learn from data and make predictions.",
  "ai": "AI stands for Artificial Intelligence — it includes ML, deep learning, and more.",
  "artificial intelligence": "AI stands for Artificial Intelligence — it includes ML, deep learning, and more.",
  "dl": "DL means Deep Learning. It uses neural networks for tasks like image and speech recognition.",
  "deep learning": "Deep Learning uses neural networks to learn from complex data like images or audio.",
  "feature": "A feature is an input variable used to make predictions.",
  "features": "A feature is an input variable used to make predictions.",
  "label": "A label is the actual outcome the model is trying to predict.",
  "labels": "A label is the actual outcome the model is trying to predict.",
  "mlgpt": "MLGPT helps you apply machine learning to your own datasets easily!",
  "why this model": "Different models work better for different types of data. You can experiment and compare!",
  "choose model": "Different models work better for different types of data. Try them out and see!",
  "pandas": "Pandas is a Python library for handling data tables and performing data manipulation.",
  "numpy": "NumPy is used for fast numerical operations and handling arrays in Python.",
  "matplotlib": "Matplotlib (plt) is used to visualize data using charts like bar plots, line graphs, etc.",
  "plt": "‘plt’ is short for matplotlib.pyplot, used for drawing graphs and charts in ML.",
  "graph": "Use matplotlib or seaborn to draw charts and graphs.",
  "visualize": "Use matplotlib or seaborn to draw charts and graphs.",
  "sklearn": "Scikit-learn is a machine learning library that provides tools to build and evaluate models.",
  "model selection": "Model selection helps choose the best ML algorithm based on accuracy and performance.",
  "cross validation": "Cross-validation helps test model performance on different parts of data.",
  "split data": "Use train_test_split from sklearn.model_selection to split data into train and test sets.",
  "train test split": "Use train_test_split from sklearn.model_selection to split data into train and test sets.",
  "label encoding": "LabelEncoder from sklearn can convert text labels into numbers for ML models.",
  "encode": "LabelEncoder from sklearn can convert text labels into numbers for ML models.",
  "decision tree": "A Decision Tree splits data into branches based on feature values to predict outcomes. It's easy to understand and visualize.",
  "svm": "Support Vector Machine (SVM) is a powerful classifier that finds the best boundary to separate different classes in data.",
  "random forest": "Random Forest combines multiple decision trees to reduce overfitting and improve accuracy. It's great for complex datasets.",
  "logistic regression": "Logistic Regression predicts binary outcomes using a logistic function. It’s used for yes/no or true/false predictions.",
  "knn": "K-Nearest Neighbors classifies data based on the most common class among its 'k' closest neighbors. It’s simple and effective.",
  "naive bayes": "Naive Bayes is a fast classifier based on Bayes’ theorem. It works well with text data like spam detection.",
  "gradient boosting": "Gradient Boosting builds models one after the other, each correcting errors of the previous one. It’s great for high accuracy.",
  "adaboost": "AdaBoost focuses on the mistakes of previous models and gives them more weight in the next round. It's used for both classification and regression.",
  "extra trees": "Extra Trees builds multiple randomized trees and averages their outputs. It's faster than Random Forest in many cases.",
  "linear regression": "Linear Regression fits a straight line to the data and is used to predict continuous values like price, temperature, etc.",
  "xgboost": "XGBoost is an optimized version of gradient boosting and is often used in Kaggle competitions due to its high performance.",
  "lightgbm": "LightGBM is a fast gradient boosting library that uses less memory and is suitable for large datasets.",
  "ensemble learning": "Ensemble methods combine multiple models to get better performance. Examples: Random Forest, AdaBoost, Gradient Boosting.",
  "train test split": "train_test_split splits your dataset into training and testing parts to evaluate model performance before deploying.",
  "label encoding": "Label Encoding turns categorical text into numbers so that ML models can work with them.",
  "one hot encoding": "One-Hot Encoding creates separate columns for each category and assigns 0 or 1. Useful for nominal data.",
  "overfitting": "Overfitting happens when a model learns the training data too well and performs poorly on new data. Use cross-validation or regularization to fix it.",
  "underfitting": "Underfitting means the model is too simple to capture the pattern. Try adding more features or use a complex model.",
  "pandas": "Pandas is a Python library for handling tabular data. It provides DataFrames to load, manipulate, and analyze datasets easily.",
  "numpy": "NumPy is a core library for numerical operations in Python. It supports arrays, matrices, and many mathematical functions.",
  "sklearn": "Scikit-learn (sklearn) is a machine learning library in Python. It includes tools for classification, regression, and clustering.",
  "model accuracy": "Accuracy tells how often the model is correct. It is the ratio of correct predictions to total predictions.",
  "confusion matrix": "A confusion matrix shows the number of true/false positives and negatives, helping to evaluate classification models.",
  "cross validation": "Cross-validation splits your data into multiple parts to test the model multiple times, giving a more reliable performance measure.",
  "precision": "Precision is the percentage of correct positive predictions. It’s useful when false positives are costly.",
  "recall": "Recall is the percentage of actual positives correctly predicted. It’s important when false negatives are risky.",
  "f1 score": "F1 Score is the harmonic mean of precision and recall. It balances both in a single metric.",
  "hyperparameter tuning": "Tuning adjusts model settings like learning rate or tree depth to improve performance.",
  "grid search": "Grid Search tests different combinations of parameters to find the best-performing model.",
  "feature scaling": "Feature scaling standardizes features to a common scale so models like SVM or KNN perform better.",
  "normalization": "Normalization scales features to [0,1] range. It’s good for algorithms sensitive to magnitudes.",
  "standardization": "Standardization makes features have mean 0 and standard deviation 1. Useful for models like SVM or Logistic Regression.",
  "ml pipeline": "An ML pipeline chains data preprocessing, model training, and evaluation steps into one smooth process."
}
